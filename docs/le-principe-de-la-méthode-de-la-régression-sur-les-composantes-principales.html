<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Le principe de la méthode de la régression sur les composantes principales | Prédire le prix des maisons présentant plusieurs caractéristiques</title>
  <meta name="description" content="4 Le principe de la méthode de la régression sur les composantes principales | Prédire le prix des maisons présentant plusieurs caractéristiques" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Le principe de la méthode de la régression sur les composantes principales | Prédire le prix des maisons présentant plusieurs caractéristiques" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Le principe de la méthode de la régression sur les composantes principales | Prédire le prix des maisons présentant plusieurs caractéristiques" />
  
  
  

<meta name="author" content="Axel-Cleris Gailloty" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="modélisations.html"/>
<link rel="next" href="comparaison-des-performances-des-différentes-méthodes-destimation.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="introduction-générale.html"><a href="introduction-générale.html"><i class="fa fa-check"></i><b>1</b> Introduction générale</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction-générale.html"><a href="introduction-générale.html#présentation-des-données"><i class="fa fa-check"></i><b>1.1</b> Présentation des données</a></li>
<li class="chapter" data-level="1.2" data-path="introduction-générale.html"><a href="introduction-générale.html#objectifs-de-létude"><i class="fa fa-check"></i><b>1.2</b> Objectifs de l’étude</a></li>
<li class="chapter" data-level="1.3" data-path="introduction-générale.html"><a href="introduction-générale.html#plan-de-létude"><i class="fa fa-check"></i><b>1.3</b> Plan de l’étude</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="analyse-exploratoire-des-données.html"><a href="analyse-exploratoire-des-données.html"><i class="fa fa-check"></i><b>2</b> Analyse exploratoire des données</a><ul>
<li class="chapter" data-level="2.1" data-path="analyse-exploratoire-des-données.html"><a href="analyse-exploratoire-des-données.html#statistiques-descriptives-univariées"><i class="fa fa-check"></i><b>2.1</b> Statistiques descriptives univariées</a></li>
<li class="chapter" data-level="2.2" data-path="analyse-exploratoire-des-données.html"><a href="analyse-exploratoire-des-données.html#statistiques-descriptives-bivariées"><i class="fa fa-check"></i><b>2.2</b> Statistiques descriptives bivariées</a></li>
<li class="chapter" data-level="2.3" data-path="analyse-exploratoire-des-données.html"><a href="analyse-exploratoire-des-données.html#lanalyse-en-composantes-principales"><i class="fa fa-check"></i><b>2.3</b> L’analyse en composantes principales</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="modélisations.html"><a href="modélisations.html"><i class="fa fa-check"></i><b>3</b> Modélisations</a><ul>
<li class="chapter" data-level="3.1" data-path="modélisations.html"><a href="modélisations.html#transformations-des-données"><i class="fa fa-check"></i><b>3.1</b> Transformations des données</a></li>
<li class="chapter" data-level="3.2" data-path="modélisations.html"><a href="modélisations.html#modèle-de-régression-multiple"><i class="fa fa-check"></i><b>3.2</b> Modèle de régression multiple</a><ul>
<li class="chapter" data-level="3.2.1" data-path="modélisations.html"><a href="modélisations.html#le-test-dinflation-de-la-variance-vif"><i class="fa fa-check"></i><b>3.2.1</b> Le test d’inflation de la variance (VIF)</a></li>
<li class="chapter" data-level="3.2.2" data-path="modélisations.html"><a href="modélisations.html#vérifions-lhypothèse-de-normalité-des-résidus"><i class="fa fa-check"></i><b>3.2.2</b> Vérifions l’hypothèse de normalité des résidus</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="modélisations.html"><a href="modélisations.html#régressions-sur-variables-latentes"><i class="fa fa-check"></i><b>3.3</b> Régressions sur variables latentes</a><ul>
<li class="chapter" data-level="3.3.1" data-path="modélisations.html"><a href="modélisations.html#pourquoi-faire-une-régression-sur-les-variables-latentes"><i class="fa fa-check"></i><b>3.3.1</b> Pourquoi faire une régression sur les variables latentes ?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="le-principe-de-la-méthode-de-la-régression-sur-les-composantes-principales.html"><a href="le-principe-de-la-méthode-de-la-régression-sur-les-composantes-principales.html"><i class="fa fa-check"></i><b>4</b> Le principe de la méthode de la régression sur les composantes principales</a><ul>
<li class="chapter" data-level="4.1" data-path="le-principe-de-la-méthode-de-la-régression-sur-les-composantes-principales.html"><a href="le-principe-de-la-méthode-de-la-régression-sur-les-composantes-principales.html#régression-sur-composantes-principales-rcp-ou-pcr"><i class="fa fa-check"></i><b>4.1</b> Régression sur composantes principales (RCP ou PCR)</a></li>
<li class="chapter" data-level="4.2" data-path="le-principe-de-la-méthode-de-la-régression-sur-les-composantes-principales.html"><a href="le-principe-de-la-méthode-de-la-régression-sur-les-composantes-principales.html#résultats-destimations-avec-sas"><i class="fa fa-check"></i><b>4.2</b> Résultats d’estimations avec SAS</a><ul>
<li class="chapter" data-level="4.2.1" data-path="le-principe-de-la-méthode-de-la-régression-sur-les-composantes-principales.html"><a href="le-principe-de-la-méthode-de-la-régression-sur-les-composantes-principales.html#dimension-1"><i class="fa fa-check"></i><b>4.2.1</b> Dimension 1</a></li>
<li class="chapter" data-level="4.2.2" data-path="le-principe-de-la-méthode-de-la-régression-sur-les-composantes-principales.html"><a href="le-principe-de-la-méthode-de-la-régression-sur-les-composantes-principales.html#dimension-2"><i class="fa fa-check"></i><b>4.2.2</b> Dimension 2</a></li>
<li class="chapter" data-level="4.2.3" data-path="le-principe-de-la-méthode-de-la-régression-sur-les-composantes-principales.html"><a href="le-principe-de-la-méthode-de-la-régression-sur-les-composantes-principales.html#dimension-3"><i class="fa fa-check"></i><b>4.2.3</b> Dimension 3</a></li>
<li class="chapter" data-level="4.2.4" data-path="le-principe-de-la-méthode-de-la-régression-sur-les-composantes-principales.html"><a href="le-principe-de-la-méthode-de-la-régression-sur-les-composantes-principales.html#dimension-4"><i class="fa fa-check"></i><b>4.2.4</b> Dimension 4</a></li>
<li class="chapter" data-level="4.2.5" data-path="le-principe-de-la-méthode-de-la-régression-sur-les-composantes-principales.html"><a href="le-principe-de-la-méthode-de-la-régression-sur-les-composantes-principales.html#dimension-5"><i class="fa fa-check"></i><b>4.2.5</b> Dimension 5</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="le-principe-de-la-méthode-de-la-régression-sur-les-composantes-principales.html"><a href="le-principe-de-la-méthode-de-la-régression-sur-les-composantes-principales.html#limite-de-la-régression-sur-les-composantes-principales"><i class="fa fa-check"></i><b>4.3</b> Limite de la régression sur les composantes principales</a></li>
<li class="chapter" data-level="4.4" data-path="le-principe-de-la-méthode-de-la-régression-sur-les-composantes-principales.html"><a href="le-principe-de-la-méthode-de-la-régression-sur-les-composantes-principales.html#régéressions-sur-les-moindres-carrés-partiels"><i class="fa fa-check"></i><b>4.4</b> Régéressions sur les moindres carrés partiels</a><ul>
<li class="chapter" data-level="4.4.1" data-path="le-principe-de-la-méthode-de-la-régression-sur-les-composantes-principales.html"><a href="le-principe-de-la-méthode-de-la-régression-sur-les-composantes-principales.html#les-loadings-et-les-weights"><i class="fa fa-check"></i><b>4.4.1</b> Les loadings et les weights</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="comparaison-des-performances-des-différentes-méthodes-destimation.html"><a href="comparaison-des-performances-des-différentes-méthodes-destimation.html"><i class="fa fa-check"></i><b>5</b> Comparaison des performances des différentes méthodes d’estimation</a></li>
<li class="chapter" data-level="6" data-path="bibliographie.html"><a href="bibliographie.html"><i class="fa fa-check"></i><b>6</b> Bibliographie</a></li>
<li class="chapter" data-level="7" data-path="annexes.html"><a href="annexes.html"><i class="fa fa-check"></i><b>7</b> Annexes</a><ul>
<li class="chapter" data-level="7.1" data-path="annexes.html"><a href="annexes.html#annexe-1-liste-exhaustive-des-colonnes"><i class="fa fa-check"></i><b>7.1</b> Annexe 1 : Liste exhaustive des colonnes</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Prédire le prix des maisons présentant plusieurs caractéristiques</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="le-principe-de-la-méthode-de-la-régression-sur-les-composantes-principales" class="section level1">
<h1><span class="header-section-number">4</span> Le principe de la méthode de la régression sur les composantes principales</h1>
<p>Dans le cadre d’une régression multiple, nous cherchons à expliquer l’endogène y à l’aide de variables explicatives X. Nous cherchons donc <span class="math inline">\(y = \alpha + \beta . X\)</span></p>
<p>La régression sur les composantes principales consiste à remplacer la matrice des <span class="math inline">\(X\)</span> (n lignes et m colonnes) par une nouvelle matrice T dérivée de X, de n lignes et k colonnes, avec k très inférieur à m.</p>
<p>Chaque colonne k doit être une combinaison linéaire des variables d’origine.</p>
<p><span class="math display">\[T= X.W\]</span></p>
<p>Avec <span class="math inline">\(W\)</span> la matrice (m,k) des coefficients définissant les combinaisons linéaires, T une matrice dont les colonnes forment des variables « artificielles » obtenues par combinaison linéaire des variables
d’origine.<br />
Une régression multiple est ensuite appliquée avec T à la place de X :
<span class="math display">\[y = c + d.T\]</span></p>
<p>Il faut donc déterminer <span class="math inline">\(W\)</span> de manière à avoir une matrice de variables explicatives T plus adaptée au calcul de la régression que la matrice X d’origine.</p>
<p>Deux méthodes pour construire T (et donc W) : RCP et RPLS</p>
<div id="régression-sur-composantes-principales-rcp-ou-pcr" class="section level2">
<h2><span class="header-section-number">4.1</span> Régression sur composantes principales (RCP ou PCR)</h2>
<p>ACP puis régression multiple. L’ACP nécessite des variables quantitatives. A partir des variables X centrées, l’ACP normée donne la matrice T dont les colonnes sont les composantes principales.<br />
Elles sont orthogonales entre elles (plus de problème de colinéarité). La matrice <span class="math inline">\(W\)</span> est la matrice des coefficients des combinaisons linéaires (coordonnées sur les composantes principales).</p>
<p>Pour cette méthode, nous allons utiliser conjointement SAS et R afin de ressortir le maximum d’informations utiles à l’interprétation des résultats.</p>
</div>
<div id="résultats-destimations-avec-sas" class="section level2">
<h2><span class="header-section-number">4.2</span> Résultats d’estimations avec SAS</h2>
<p>Nous avons spécifié la méthode “PCR” dans le procédure PLS afin de réaliser une régression sur les composantes principales. Il y a en total 34 variables explicatives pour 2930 observations.<br />
Nous choisissons également de faire une validation croisée sur le jeu de données.</p>
<p><img src="pcr-img/pcr_general_info.PNG" /></p>
<p><img src="pcr-img/pcr_split-sample.PNG" /></p>
<p>Pour pouvoir déterminer avec confiance le nombre de composantes principales (ou facteurs) à retenir dans la PCR, l’algorithme utilise une technique de validation croisée. Une technique de validation croisée consiste à diviser l’échantillon en plusieurs groupes sur lesquels des ajustements successifs sont effectués. L’ajustement du modèle est effectué sur le groupe d’apprentissage et les résultats sont comparés aux observations du groupe test. L’efficaité prédictive est évaluée à l’aide l’indicateur PRESS (Predicted REsidual Sum of Squares).</p>
<p>Le tableau suivant nous montre que la valeur minimale de la validation croisée est atteinte si le nombre de composantes principales est 8. Toutefois il est préférable de se fier au test qui indique si oui ou non il existe une différence significative entre les 8 composantes.</p>
<p>Le test nous indique qu’à partir de la 4e composante il n’existe plus de différences significatives entre les composantes. Nous allons donc retenir 5 composantes pour la suite de l’analyse.</p>
<p><img src="pcr-img/pcr_analysis_val_crois.png" /></p>
<p>Il convient maintenant d’interpréter les résultats du modèle construit.<br />
Le tableau suivant affiche la variation de pourcentage expliquée par composantes principales</p>
<p><img src="pcr-img/pcr_results.PNG" /></p>
<p>Le tableau affiche deux catégories de résultats : la variation de pourcentage expliquée par composantes principales sur les effets du modèle (les variables explicatives) et la variation de pourcentage expliquée de la variable dépendante pour chaque facteur (ou composante) ajouté.</p>
<p>Nous voyons donc que pour les 5 composantes retenues, la régression sur les composantes principales explique 75.93% de la variable dépendante et environ 43% des variables explicatives.</p>
<p>Cela signifie que les 5 composantes retenues qui arrivent à expliquer 43% de la variance des variables explicatives sont capables d’expliquer 75.93% de la variance des prix.</p>
<p>Les résultats de SAS ne nous donnent pas les coefficients associés à chaque composante. Nous pouvons donc les estimer avec R en utilisant le même nombre de composantes que la validation croisée avec SAS a indiqué.</p>
<pre><code>
Call:
lm(formula = Sale_Price ~ ., data = coord_pc)

Residuals:
    Min      1Q  Median      3Q     Max 
-535355  -20542   -3320   17245  304779 

Coefficients:
             Estimate Std. Error  t value      Pr(&gt;|t|)    
(Intercept) 180796.06     724.62 249.5033     &lt; 2.2e-16 ***
Dim.1        27395.65     291.64  93.9374     &lt; 2.2e-16 ***
Dim.2        -7046.03     415.69 -16.9502     &lt; 2.2e-16 ***
Dim.3          478.57     497.63   0.9617        0.3363    
Dim.4         2778.86     513.24   5.4144 0.00000006649 ***
Dim.5         5769.77     628.86   9.1750     &lt; 2.2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 39224 on 2924 degrees of freedom
Multiple R-squared:  0.75934,   Adjusted R-squared:  0.75893 
F-statistic: 1845.2 on 5 and 2924 DF,  p-value: &lt; 2.22e-16</code></pre>
<p>Les résultats globaux du modèle que nous trouvons sont les mêmes avec R. Les 5 composantes retenues expliquent bien 75,93% de la variance des prix de vente de la maison.</p>
<p>Nous pouvons écrire ce résultat sous forme d’une équation :</p>
<p><span class="math display">\[SalePrice = 1800796.06 + 27395.65 * Dim1 - 7046.03 * Dim2 + 478.57 * Dim3 + 2778.86* Dim4 + 5769.77* Dim5 \]</span>
Tous les coefficients sont positifs, à l’exception du coefficient de la dimension 2.
Le coefficient de la dimension 3 est le plus faible et est non significatif (p-value &gt; 5%).</p>
<p>Pour pouvoir interpréter les coefficients, il importe de savoir ce que mesure chaque composante. A l’aide de R nous pouvons, comme nous l’avons fait dans la section 2.3, afficher ce que mesure chaque composante en affichant les 10 premières contributions à chaque composante.</p>
<p>Nous décidons de représenter ces composantes sur un graphique.</p>
<p>Les contributions des variables aux différents axes factoriels sont :</p>
<p><img src="main_files/figure-html/unnamed-chunk-24-1.png" width="4800" /></p>
<p>Le poids des variables sur les trois premières dimensions est assez équilibrée. Les deux dernières dimensions sont chacune fortement dominées par une variable.</p>
<p><img src="main_files/figure-html/unnamed-chunk-25-1.png" width="2400" /></p>
<div id="dimension-1" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Dimension 1</h3>
<p>La dimension 1 fait référence à la superficie de la maison, que ce soit la taille de la résidence et des pièces qui composent la maison. Cette composante est également dans une certaine mesure influencée par l’âge de la maison (l’année où la maison a été construite et l’année où elle a été renovée.)</p>
</div>
<div id="dimension-2" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Dimension 2</h3>
<p>La deuxième dimension se refère aux caractéristiques des maisons telles que la superficie du deuxième étage, et le nombres de pièces (salles de bain, chambres, cuisines) se trouvant au deuxième étage. Cette composante détermine donc le prestige de la maison.</p>
</div>
<div id="dimension-3" class="section level3">
<h3><span class="header-section-number">4.2.3</span> Dimension 3</h3>
<p>La troisième composante mesure l’âge de la maison et la condition de la maison au moment de la vente.</p>
</div>
<div id="dimension-4" class="section level3">
<h3><span class="header-section-number">4.2.4</span> Dimension 4</h3>
<p>Cette composante est principalement dominée par le fait que la maison contient des parties inachevées.</p>
</div>
<div id="dimension-5" class="section level3">
<h3><span class="header-section-number">4.2.5</span> Dimension 5</h3>
<p>Cette composante est fortement dominée par le fait s’il y a une cuisine à l’étage et par la condition générale de la maison.</p>
</div>
</div>
<div id="limite-de-la-régression-sur-les-composantes-principales" class="section level2">
<h2><span class="header-section-number">4.3</span> Limite de la régression sur les composantes principales</h2>
<p>Nous venons de voir l’intérêt de la régression sur les composantes principales. Nous avons vu notamment que les composantes issues de l’ACP sont orthogonales en ce sens qu’il cette méthode permet d’obtenir des composantes orthogonales entre elles. Toutefois, l’analyse en composante principale que nous avons effectuée maximise les variances des variables <span class="math inline">\(X\)</span> indépendamment des variations de Y : la variance de Y n’est pas maximisée. Ceci peut surestimer le poids de certaines explicatives et conduire
de ce fait à de mauvaises interprétations des résultats.</p>
<p>Nous pouvons donc utiliser la deuxième méthode de cette famille.</p>
</div>
<div id="régéressions-sur-les-moindres-carrés-partiels" class="section level2">
<h2><span class="header-section-number">4.4</span> Régéressions sur les moindres carrés partiels</h2>
<ol start="2" style="list-style-type: decimal">
<li>Régressions PLS
Deux modèles PLS : PLS simple (ou PLS 1) et PLS 2</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>PLS 1
Comme pour la régression PCR, calcul de T dont les éléments
sont les « scores » et les colonnes les « composantes » : T =
XW avec W matrice des poids (ou loadings) et X matrice des
variables explicatives centrées (n,m).</li>
</ol>
<p>Mais, contrairement à la PCR, le calcul de T se fait en tenant compte de la variable à prédire <span class="math inline">\(y\)</span>. Double modélisation :</p>
<p>X = TP + R (1)
Y = TQ + F (2)</p>
<p>Avec : R matrice des résidus associées à la prédiction de X ;
F vecteur des résidus associé à la prédiction de Y.</p>
<p>Première étape : calculer t1 la première composante principale, puis estimer (1) et (2) à une seule composante :</p>
<p><span class="math inline">\(X = t_1 . p_1 + R1\)</span> et <span class="math inline">\(Y = t_1 . q_1 + F_1\)</span></p>
<p>Avec <span class="math inline">\(t_1\)</span> de dimension (n,1)
<span class="math inline">\(p_1\)</span> de dim (1,m) loadings
<span class="math inline">\(q_1\)</span> de dim (1,1) loadings</p>
<p>On introduit une deuxième composante <span class="math inline">\(t_2\)</span>. Ainsi, une ligne <span class="math inline">\(x_i\)</span> de X est égale à : <span class="math inline">\(x_i = t_1i p_1 + t_2i p2\)</span></p>
<p>L’introduction de nouvelles composantes se fait selon la même procédure : partant d’un modèle à k composantes, on créé un nouveau modèle à k+1 composantes en calculant une nouvelle composante tk, puis les paramètres des deux modèles couplés :
<span class="math inline">\(X = t_1p_1+t_2 p_2 +…+t_k p_k + t_k_+1 p_k+1 + R_k+1\)</span>
<span class="math inline">\(Y = t_1q_1+t_2 q_2 +…+t_k q_k + t_k+1 q_k+1 + F_k+1\)</span></p>
<div id="les-loadings-et-les-weights" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Les loadings et les weights</h3>
<div style="page-break-after: always;"></div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="modélisations.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="comparaison-des-performances-des-différentes-méthodes-destimation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
